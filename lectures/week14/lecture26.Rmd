---
title: "Lecture 26"
author: "Peter Shaffery"
date: "4/8/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warnings=FALSE, message=FALSE)
```

# Introduction to Missing Data

*Lectures follow chapters of "Statistical Analysis with Missing Data" (Little and Rubin, 2002)*

One of the most common problems "data problems" you will face in statistical analysis is *missing data*. Missing data are what they sound like, datapoints which have incomplete information about one or more units in our analysis. 

Consider, for example, the titanic dataset

```{r}
library(tidyverse)
library(magrittr)
library(rstanarm)

dat=read.csv('../../data/titanic.csv')
```


# Computational Side 

Missing data is commonly represented in computer records by the letters `NA`, meaning "Not Available". R treats `NA` as a special quantity, which has it's own rules and operations:
```{r}
NA + 1
NA == 1
NA == NA
c(NA,1)
paste(NA,'foo',sep='')
```

Notice in that last example, the command `paste` (which concatenates strings, `paste('a','b',sep=',')` outputs `"a,b"`) doesnt treat `NA` as "missing data", but actually converts it to a string. This is one of the most annoying things that can happen with `NA`s: they will sometimes be converted (or recorded) as "string NAs", `"NA"` (or even worse, `"na"`). This *does not* have the special properties of `NA` and will be the bane of your existence if you're not careful about checking for them:
```{r}
is.na('NA')
is.na(NA)
```
One way to keep track of all these is to use a judicious application `dplyr::na_if`:
```{r}
bad.dat = data.frame(
  'idx'=c(1,2,3),
  'missing1'=c(NA,'a','b'),
  'missing2'=c('what','the','na')
  )

bad.dat %>% 
  na_if('na')
```

To further complicate matters, sometimes `NA`s will be represents using `NaN` (or `Nan` or `nan`), meaning "Not a Number".

```{r}
is.nan(NaN)
is.nan(NA)
```

This one is a pain to handle:
```{r}
nightmare.dat = data.frame(
  'idx'=c(1,2,3),
  'missing1'=c(NA,'a','b'),
  'missing2'=c(2.43,NaN,31.4),
  'missing3'=c('what','the','na')
  )

nightmare.dat %>% 
  na_if(NaN) %>%
  na_if('na')

```

In general, it's fine to just keep true `NaN`s as they occur, because in most situations they will behave sufficiently like `NA`s to not cause problems, but be aware that these are different concepts.

Once you've cleaned up your `NA`s, the next step is to determine how prevalent they are. One very useful tool for this are "missing data patterns":
```{r}
library(mice) # "Multiple Imputation by Chained Equations
library(VIM) # Visualization and Imputation of Missing Data

aggr(dat) # VIM version
md.pattern(dat, rotate.names=TRUE) # mice version

dat %<>% drop_na(-age)
```

In the `titanic` dataset we see that AGE variable has the highest amount of missingness by far.

# Statistical Models with Missing Data

In the past we've dealt with missingness by simply dropping the rows that have missing values:

```{r}
# clearly age matters
mod = glm(survived ~ sex + as.factor(pclass) + sibsp + age + fare, data=dat%>%drop_na, family=binomial)
summary(mod)
```

This strategy for dealing with missing data analysis is referred to as **complete case analysis**. While it is certainly the *easiest* way of dealing with missing data, it does have some drawbacks.

For one, we're dumping a lot of data, 263 rows is like 20\% of the dataset, and that's kind of as waste. Moreover, we may be losing important information:
```{r}
dat %>% group_by(sex,pclass) %>% summarize(sum(is.na(age)))
```

The missingness here is imbalanced across categories! Therefore the decision to ignore the missing data could bias our inference regarding these (or other) independent variables.

What we need is a technique that will allow us to handle the missingness of some data. There are many possible ways that we could do so. The choice of method, and it's ultimate efficacy, hinges on the **missing data mechanism**; how the data came to be missing in the first place.

# Missingness Mechanisms

Missingness mechanisms fall into three important categories:

1. **Not Missing at Random (NMAR)**
2. **Missing At Random (MAR)**
3. **Missing Completely at Random (MCAR)**

In order to define these categories rigorously, we have to first introduce some notation.

Say that we have a dataset $D$, which includes both our vector of dependent variables (outcomes) $\vec y = [y_1,...,y_n]^T$, as well as our matrix of independent variables $X = [\vec x_1,...,\vec x_n]^T$. Furthermore, say that for some of the observations $D_i = \texttt{NA}$, which means that $y_i$ or an element(s) of $\vec x_i$ are missing.

Now, we'll define a second "missingness vector" $\vec m = [m_1,...,m_n]^T$, where $m_i=\begin{cases}1, D_i = \texttt{NA}\\0, D_i \neq \texttt{NA}\\  \end{cases}$. We will interpret $\vec m$ as a "mask" for the data: when $m_i=1$ we are prevented from knowing the true value of $D_i$. Furthermore, we will sometimes use $\vec D_{\text{obs}}$ and $\vec D_{\text{mis}}$ to denote the observed and missing components of the data.

The reason we define the missingness vector $\vec m$ separately, is that it allows us to endow it with a probability distribution which depends both on a parameter(s) $\phi$, as well as on the data itself $\vec y$:

$$
\vec m \sim P[ \vec m | D, \phi]
$$
Thinking about missing data in this way, as a random variable with a distribution that can be estimated or learned about, is the core of statistical imputation. Moreover, we can use the specific form of $P[ \vec m | \vec y, \phi]$ to differentiate between different types of missingness mechanisms.

## Not Missing at Randoms

NMAR data is the worst case, and depending on context may just not be addressable from a statistical point of view. NMAR data is defined by a missingness mechanism of the form:

$$
\vec m \sim P[ \vec m | \vec D_{\text{mis}}, \phi]
$$
A simple example of this would be $y_i \sim N(\mu,\sigma^2)$, where we want to estimate $\mu$ and $\sigma^2$. Now, let's say that due to a flaw in our measuring tool, there is a threshold $\tau$, and if $y_i>\tau$ then $y_i$ is set equal to `NA`. Clearly:

$$
m_i \sim P[y_i > \tau] = \int_{\tau}^{\infty} N(y_i; \mu, \sigma^2) d y_i
$$

For a more realistic example, say that we are performing some experiment on mice in three age groups, and that we expect age to be an important determinant of individuals' response to the treatment. If our new lab intern screws up some component of the data collection process, and all of the samples in the oldest age category are contaminated, there's not much you can do to recover that data.

NMAR data can be further broken down into two cases: data which is *censored* (where the specifics of the missing data mechanism, eq. $\tau$) are known, and data which are not censored. 

Censored data can sometimes be dealt with by incorporating the knowledge of the censoring mechanism into the model likelihood. One example of this is longitudinal medical studies, where patients who survive long enough often just drop out of a study. You can still use their last known survival time as a component in the analysis.

On the other hand, if the censoring mechanism is unknown to you, then you're pretty much out of luck. In this case the only recourse (besides collecting new data) is complete case analysis: dropping the portions of the data which were excluded, and being explicit about that constraint in your analysis. 


## Missing Completely at Random

At the other end of the spectrum is MCAR data. This is defined by a missingness mechanism that has no dependence on the data:

$$
\vec m \sim P[\vec m | \phi]
$$
MCAR data is the best case, because it impacts your sample size and nothing else. Consider again the example of our clumsy intern, except this time it's just a random 10% of the individual outcomes which are contaminated, rather than any particular age group. Since the samples in each age group are iid, this is effectively just a reduction in the number of measurements within each group. So long as our initial sample size was sufficiently large, we should be fine to just ignore the missing data and perform a complete case analysis. 

## Missing at Random

Most of the time missing data will not fall into either extreme, but will instead be just MAR. Data which is MAR has the missingness mechanism:

$$
\vec m \sim P[\vec m| D_{\text{obs}},\phi]
$$
That is, the missingess of either $y_i$ or $m_i$, depends on some variable which we have measured and which is never missing.

One example of this is our `titanic` dataset:
```{r}
dat %>% group_by(sex,pclass) %>% summarize(sum(is.na(age)))
```

We see that the missingess of AGE has clear dependence on the variables SEX and PCLASS, both of which are completely observed (except for like one case which we'll just ignore).

Another way that we could assess this is through a margin plot:
```{r}
marginplot(dat[,c('age','fare')])
marginplot(dat[,c('age','pclass')])
```

While with NMAR and MAR data our main tool was complete case analysis (or special methods, in the case of censored NMAR data), with MCAR data we have a few other options which are more appropriate:

1. *Weighting Methods:* Estimate $\pi_i = P[m_i| D_{\text{obs}},\phi]$, and then use it to weight the data in some smart way. For example, say that we have iid normal data $y_i$, with dependent variable $x_i$ which is equal to 0 or 1. Say that the missingness mechansim is $P[m_i=1|x_i=0] = .25$ and $P[m_i=1|x_i=1] = .75$. The "correct" estimate of the normal mean $\mu$, is the weighted sample mean $\hat mu = \frac{1}{n} \sum_i \bar \pi_i^{-1} y_i$, where $\bar \pi_i$ are just the normalized weights $\pi_i = \frac{\pi_i}{\sum_i \pi_i}$. You will sometimes hear $\pi_i$ referred to as the **design weights** (this is a common technique in survey analysis to deal with nonresponse)

2. *Imputation Methods:* Use your knowledge $P[m_i| D_{\text{obs}},\phi]$ to generate samples of *synthetic data*, $\hat D_{\text{mis}}$. You then treat these samples as if they were "real" measurements, and fit your model with the sampled data $\hat D = [D_{\text{obs}},\hat D_{\text{mis}}]$ . Typically this is done as part of a **multiple imputation** strategy, where you create a few different samples $\hat D^{(i)}$, and then you average the resulting model coefficients obtained from each $\hat D^{(i)}$ This is the strategy that we will focus on for the rest of lecture.

3. *Model-Based Methods:* Incorporate your knowledge of $P[m_i| D_{\text{obs}},\phi]$ directly into your model likelihood (like with the censored data mentioned above). This is a very rich approach to missing data problems, but because its so contextual we won't be able to give it much space here. For a detailed discussion check out Parts II and III of the textbook mentioned at the top of the lecture.

# Imputation

The idea with imputation is that we will "fill-in" the missing data with some reasonable guesses, and then fit our model to the filled-in dataset. What differentiates the varieties of imputation is *how* you fill in the data. Very broadly, these method fall into two classes: **explicit** method, where you actually fit a statistical model for the missing data, and **implicit** methods, where you use observed data as a "surrogate" for the model.

## Explicit Methods

### Mean Imputation

The simplest way of imputing missing data is to just construct some suitable subgroups in your data, and replace missing data with subgroup-level means. For the titanic dataset, a crude implementation of this strategy would look like:

```{r}
age.mn = dat %>%
  group_by(sex,pclass) %>%
  transmute(age.mn = mean(age,na.rm=TRUE)) %>%
  ungroup %>%
  select(age.mn)

dat$age.imputed = coalesce(dat$age,unlist(age.mn))

mod.mean.imputed = glm(survived ~ sex + as.factor(pclass) + sibsp + age.imputed + fare, data=dat, family=binomial)
coefficients(mod)
coefficients(mod.mean.imputed)
```


### Regression Imputation

Fit a regression model for the missing variable(s) to some other variables of interest, and then impute missing data with the predicted regression mean. This method is conceptually identically to the above, except it also incorporates continuous predictors, as well as categorical (ie. group labels):

```{r}
plot(dat$sibsp,dat$age) # introduce a non-categorical variable to differentiate from subgroup mean approach
age.mod = lm(age~sex+as.factor(pclass)+sibsp, data=dat%>%drop_na)
age.reg = predict(age.mod,newdata=dat)

dat$age.imputed = coalesce(dat$age,age.reg)

mod.reg.imputed = glm(survived ~ sex + as.factor(pclass) + sibsp + age.imputed + fare, data=dat, family=binomial)
coefficients(mod)
coefficients(mod.reg.imputed)
```

### Stochastic Regression Imputation

As above, fit a regression model for the missing variable(s). However rather than using predicted regression mean, draw a sample from the full forecasting distribution for each missing datapoint. The idea here is to incorporate some additional uncertainty into the imputation, through the residuals.

```{r}
set.seed(12345)
age.mod = lm(age~sex+as.factor(pclass)+sibsp, data=dat%>%drop_na)
sigma = sd(resid(age.mod))
age.stoch.reg = predict(age.mod,newdata=dat) + rnorm(nrow(dat), 0, sigma)

dat$age.imputed = coalesce(dat$age,age.stoch.reg)

mod.stoch.reg.imputed = glm(survived ~ sex + as.factor(pclass) + sibsp + age.imputed + fare, data=dat, family=binomial)
coefficients(mod)
coefficients(mod.stoch.reg.imputed)

dat %<>% select(-age.imputed)
```
## Implicit Methods

### Hot Deck Imputation

The only implicit method that we will look at today is **hot deck** imputation. Unlike the explicit methods, which constructed a concrete model for the AGE variable, *implicit* method will sidestep the modeling stage. The idea here is that we will instead use the data itself as our "model". 

Say that we have a datapoint $D_{\text{mis}}^*$, which contains some missing data that we would like to impute. The hot deck approach does so in two steps:

1. Using the non-missing components of $D_{\text{mis}}^*$, select (usually at random) a **donor** datapoint $D_{\text{donor}}$ which has no missing data
2. Fill in the missing components of $D_{\text{mis}}^*$ using the corresponding components of $D_{\text{donor}}$

For a single datapoint, we might implement this as:
```{r}
set.seed(12345)

D.star = dat[is.na(dat$age),] %>% sample_n(1)

donor.sex = D.star$sex
donor.pclass = D.star$pclass
candidate.donor = (dat$sex==donor.sex)&(dat$pclass==donor.pclass)

D.donor = dat[candidate.donor,] %>% sample_n(1)
D.star.imputed = coalesce(D.star, D.donor)
```

Now, both steps 1 and 2 can be done in a lot of different ways. Donor candidates are usually select from a pool of "similar" candidates, but similarity can be measured in a lot of different ways. Similarly, how you pick a donor once you've got your candidate pool can vary a lot. You might choose donors weighted by similarity. You might choose different donors to impute different columns. It's pretty much all up for grabs.

In general, rather than making all these decisions yourself, it's simplest to just use some pre-designed packages:

```{r}
reg.imputed = VIM::regressionImp(age~sex+pclass+fare, data=dat)
reg.imputed %>% head

hd.imputed = VIM::hotdeck(dat,variable='age',domain_var=c('sex','pclass'))
hd.imputed %>% head

knn.imputed = VIM::kNN(dat,variable='age')
knn.imputed %>% head

marginplot(knn.imputed[,c('age','fare','age_imp')],delimiter='_imp')
```

# Multiple Imputation

## Bootstrapping

Aside from implicit vs explicit, there is one other main distinction between imputation techniques: *random* techniques (stochastic regression or hot deck), and *deterministic* techniques (regression imputation or `VIM::kNN`).

In general, while deterministic method are simpler, theoretically they have some drawbacks. Primarily, they don't give you any sense of how much your particular choice of imputed values effects your final model estimates. In some particular cases you can derive standard errors, but it adds a bit of computational baggage.

Random imputation techniques, on the other hand, do not have this drawback. By sampling multiple versions of the same imputed data, you can fit the model to each sample and get an approximate distribution of your model coefficients under the sampling distribution of the **imputed data**.

When this distribution varies a lot, it indicates that your imputation is injecting a lot of uncertainty into the model, whereas when this distribution is fairly consistent then imputation doesn't matter that much:

```{r}
impute.and.fit = function(dat){
  dat.imputed = VIM::hotdeck(dat, variable='age', domain_var = c('sex','pclass'))

  mod.imputed = glm(survived ~ sex + as.factor(pclass) + sibsp + age + fare,
                    data=dat.imputed,
                    family=binomial)
  
  return(as.numeric(coefficients(mod.imputed)))
  
}

samps = matrix(0,nrow=10,ncol=7)
for (i in 1:10){
  samps[i,] = impute.and.fit(dat)
}

apply(samps,2,mean)
apply(samps,2,sd)
```

Now, we do need to be a little careful about defining what we're estimating here. Recall that, for a single imputed dataset $D^{(i)}_{\text{imp}}$, our resulting coefficient estimate $\hat \beta^{(i)}$ has some uncertainty due to just the estimation process, which we assess using it's standard error $s.e.(\hat \beta^{(i)})$. This is **not** the same thing as the changes in $\hat \beta$ due to the imputation, which we measured above.

To distinguish between these concepts we will define the **within-imputation** variance of $\hat \beta^{(i)}$, $W_i = s.e.(\hat \beta^{(i)})^2$, and the **between-imputation** variance of, $B = \frac{1}{m} \sum_{i=1}^m (\hat \beta^{(i)} - \bar \beta)^2$ (where $\beta \beta$ is just the mean of the $\hat \beta^{(i)}$.)

Because the imputation process has introduced this extra uncertainty $B$ into our estimate $\hat \beta$, we cannot simply use $s.e.(\hat \beta_i)$ to eg. perform a hypothesis test. We need to combine $W_i$ and $B$ in an appropriate way to properly represent our uncertainty of $\hat \beta$.

The formula for this is actually quite simple:

$$
T = (\frac{1}{m} \sum_{i=1}^m W_i) + \frac{m+1}{m}B = \bar W + \frac{m+1}{m}B
$$

This is called the **total variance** of $\bar \beta$, and allows us to compute a final standard error for our mean coefficient vector $s.e.(\bar \beta) = \sqrt{T}$. For sufficiently large datasets, it can be shown that $\bar \beta$ is approximately normal, and so we can, for example, compute confidence intervals using our usual t-statistic.

While it's possible to do this all by hand, in practice it's kind of a pain, so as usual we will instead use a package's workflow, in this case `mice`:

```{r}
vars = c('survived','age','pclass','sex','sibsp')

# we don't want mice to use the survived variable to impute age, so we'll construct a predictor matrix which prevents this
pred.mat = matrix(1,nrow=length(vars),ncol=length(vars))
pred.mat = pred.mat - diag(length(vars))
pred.mat[1,] = 0
pred.mat[,1] = 0

# By default, mice uses an imputation method called Predictive Mean Matching (PMM)
# The specifics of this imputation strategy are a little complicated, but it's like hot deck in that it identifies "similar" donors and randomly uses their values to impute 
dat.imputed = mice(dat[vars], predictorMatrix=pred.mat) 
mods.imputed = with(data=dat.imputed, expr = glm(survived ~ age + as.factor(pclass) + sex + sibsp, family=binomial))
mod.mi = pool(mods.imputed)
summary(mod.mi)
```