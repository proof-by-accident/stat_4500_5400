% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
%
\documentclass[
]{article}
\usepackage{amsmath,amssymb}
\usepackage{lmodern}
\usepackage{ifxetex,ifluatex}
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math}
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
\usepackage{xcolor}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\IfFileExists{bookmark.sty}{\usepackage{bookmark}}{\usepackage{hyperref}}
\hypersetup{
  pdftitle={Lecture 12},
  pdfauthor={Peter Shaffery},
  hidelinks,
  pdfcreator={LaTeX via pandoc}}
\urlstyle{same} % disable monospaced font for URLs
\usepackage[margin=1in]{geometry}
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
% Set default figure placement to htbp
\makeatletter
\def\fps@figure{htbp}
\makeatother
\setlength{\emergencystretch}{3em} % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{-\maxdimen} % remove section numbering
\ifluatex
  \usepackage{selnolig}  % disable illegal ligatures
\fi

\title{Lecture 12}
\author{Peter Shaffery}
\date{2/21/2021}

\begin{document}
\maketitle

\hypertarget{mea-culpa-code-error-in-last-weeks-lecture}{%
\section{Mea Culpa: Code Error in Last Week's
Lecture}\label{mea-culpa-code-error-in-last-weeks-lecture}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(tidyverse)}
\FunctionTok{library}\NormalTok{(magrittr)}
\FunctionTok{library}\NormalTok{(broom)}
\FunctionTok{library}\NormalTok{(car)}

\NormalTok{educ }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}../../data/education.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{educ }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ head}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   state educ income youth urban region    gdp tax wls.weights
## 1    ME  235  39440   325   508      1 161421 233  0.00066169
## 2    NH  231  45780   323   564      1 195522  16  0.00066169
## 3    VT  270  40110   328   322      1 171952 294  0.00066169
## 4    MA  261  52330   305   846      1 239147 582  0.00066169
## 5    RI  300  47800   303   871      1 198699 253  0.00066169
## 6    CT  317  58890   307   774      1 271263 544  0.00066169
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# we don\textquotesingle{}t need the wls.weights variable for now, so drop it}
\NormalTok{educ }\SpecialCharTok{\%\textless{}\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{wls.weights)}
\NormalTok{mod.dat }\OtherTok{=}\NormalTok{ educ }\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{select}\NormalTok{(}\SpecialCharTok{{-}}\NormalTok{state)}
\NormalTok{mod }\OtherTok{=} \FunctionTok{lm}\NormalTok{(educ}\SpecialCharTok{\textasciitilde{}}\NormalTok{.,}\AttributeTok{data=}\NormalTok{mod.dat}\SpecialCharTok{\%\textgreater{}\%} \FunctionTok{mutate}\NormalTok{(}\StringTok{\textquotesingle{}region\textquotesingle{}}\OtherTok{=}\FunctionTok{as.factor}\NormalTok{(region)))}

\CommentTok{\# for an explanation of GVIF see https://stats.stackexchange.com/questions/70679/which{-}variance{-}inflation{-}factor{-}should{-}i{-}be{-}using{-}textgvif{-}or{-}textgvif/96584}
\FunctionTok{vif}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##            GVIF Df GVIF^(1/(2*Df))
## income 6.096208  1        2.469050
## youth  1.819528  1        1.348899
## urban  1.853798  1        1.361542
## region 3.136290  3        1.209863
## gdp    4.572875  1        2.138428
## tax    2.370822  1        1.539747
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(mod)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## 
## Call:
## lm(formula = educ ~ ., data = mod.dat %>% mutate(region = as.factor(region)))
## 
## Residuals:
##     Min      1Q  Median      3Q     Max 
## -66.365 -24.133  -7.134  22.274  88.263 
## 
## Coefficients:
##               Estimate Std. Error t value Pr(>|t|)   
## (Intercept) -4.201e+02  1.495e+02  -2.809  0.00758 **
## income       5.540e-03  2.154e-03   2.572  0.01383 * 
## youth        1.351e+00  3.905e-01   3.459  0.00128 **
## urban       -2.536e-02  5.279e-02  -0.480  0.63356   
## region2     -7.332e+00  2.024e+01  -0.362  0.71905   
## region3     -1.138e+01  1.849e+01  -0.615  0.54169   
## region4      1.544e+01  2.176e+01   0.710  0.48200   
## gdp         -2.041e-05  3.606e-04  -0.057  0.95513   
## tax          8.656e-02  5.168e-02   1.675  0.10158   
## ---
## Signif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1
## 
## Residual standard error: 39.36 on 41 degrees of freedom
## Multiple R-squared:  0.6555, Adjusted R-squared:  0.5883 
## F-statistic: 9.752 on 8 and 41 DF,  p-value: 1.87e-07
\end{verbatim}

\hypertarget{example-wine}{%
\section{Example: Wine}\label{example-wine}}

Recall the wine dataset that we looked at last week:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wine }\OtherTok{=} \FunctionTok{read.csv}\NormalTok{(}\StringTok{\textquotesingle{}../../data/wine.csv\textquotesingle{}}\NormalTok{)}
\NormalTok{wine[}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{] }\OtherTok{=} \DecValTok{1972}\SpecialCharTok{{-}}\NormalTok{wine}\SpecialCharTok{$}\NormalTok{vintage}
\NormalTok{wine }\SpecialCharTok{\%\textgreater{}\%}\NormalTok{ head}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
##   vintage price age
## 1    1890 50.00  82
## 2    1900 35.00  72
## 3    1920 25.00  52
## 4    1931 11.98  41
## 5    1934 15.00  38
## 6    1935 13.00  37
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{plot}\NormalTok{(wine}\SpecialCharTok{$}\NormalTok{age, wine}\SpecialCharTok{$}\NormalTok{price)}
\end{Highlighting}
\end{Shaded}

\includegraphics{lecture12_files/figure-latex/unnamed-chunk-2-1.pdf}

During our discussion of how to model the relationship between between
\(AGE\) and \(PRICE\) in this data, we arrived at the following model:
\[
\text{PRICE}_i = \exp{(\beta_0 + \beta_1 \text{AGE}_i)} \exp(\epsilon_i)
\]

One of the primary reasons we had chosen this model was hat it enabled
us to apply linear regression, after first transforming the data: \[
\ln{\text{PRICE}_i} = \beta_0 + \beta_1 \text{AGE}_i + \epsilon_i
\] This model had some drawbacks, however. Primarily, it forced us to
assume that (on the PRICE scale) the random errors were log-normally
distributed and (more importantly) \emph{multiplicative}.

Multiplicative errors are not necessarily ``wrong'', but they may be an
inappropriate modeling assumption. For example: \[
\begin{split}
\text{Var}[\text{PRICE}_i] &= \text{Var}\left[ \exp{ \left( \beta_0 + \beta_1 \text{AGE}_i \right)} \exp{\epsilon}_i  \right]\\
&= (\exp{ \left( \beta_0 + \beta_1 \text{AGE}_i \right)})^2 \text{ Var}\left[  \exp{\epsilon}_i  \right]\\
&= \exp{ 2\left( \beta_0 + \beta_1 \text{AGE}_i \right)} \text{ Var}\left[  \exp{\epsilon}_i  \right]\\
\end{split}
\] Thus (as we saw with the prediction intervals last week)
multiplicative errors imply that the variance of PRICE increases
exponentially with AGE:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{wine}\SpecialCharTok{$}\NormalTok{lprice }\OtherTok{=} \FunctionTok{log}\NormalTok{(wine}\SpecialCharTok{$}\NormalTok{price)}
\NormalTok{mod.log }\OtherTok{=} \FunctionTok{lm}\NormalTok{(lprice}\SpecialCharTok{\textasciitilde{}}\NormalTok{age, }\AttributeTok{data=}\NormalTok{wine)}

\NormalTok{plot.df }\OtherTok{=}\NormalTok{ mod.log }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{augment}\NormalTok{(}\AttributeTok{interval=}\StringTok{\textquotesingle{}prediction\textquotesingle{}}\NormalTok{) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{mutate}\NormalTok{(}\StringTok{\textquotesingle{}.fitted\textquotesingle{}}\OtherTok{=}\FunctionTok{exp}\NormalTok{(.fitted),}\StringTok{\textquotesingle{}lower\textquotesingle{}}\OtherTok{=}\FunctionTok{exp}\NormalTok{(.lower),}\StringTok{\textquotesingle{}upper\textquotesingle{}}\OtherTok{=}\FunctionTok{exp}\NormalTok{(.upper)) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{select}\NormalTok{(age,.fitted,lower,upper) }\SpecialCharTok{\%\textgreater{}\%}
  \FunctionTok{rename}\NormalTok{(}\StringTok{\textquotesingle{}exp.fit\textquotesingle{}}\OtherTok{=}\NormalTok{.fitted) }\SpecialCharTok{\%\textgreater{}\%}
\NormalTok{  reshape2}\SpecialCharTok{::}\FunctionTok{melt}\NormalTok{(}\AttributeTok{id.vars=}\FunctionTok{c}\NormalTok{(}\StringTok{\textquotesingle{}age\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}upper\textquotesingle{}}\NormalTok{,}\StringTok{\textquotesingle{}lower\textquotesingle{}}\NormalTok{))}

\NormalTok{plot.df[}\StringTok{\textquotesingle{}price\textquotesingle{}}\NormalTok{] }\OtherTok{=}\NormalTok{ wine}\SpecialCharTok{$}\NormalTok{price}

\FunctionTok{ggplot}\NormalTok{(plot.df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x=}\NormalTok{age,)) }\SpecialCharTok{+}
  \FunctionTok{geom\_line}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{value)) }\SpecialCharTok{+}
  \FunctionTok{geom\_ribbon}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{ymin=}\NormalTok{lower,}\AttributeTok{ymax=}\NormalTok{upper), }\AttributeTok{alpha=}\NormalTok{.}\DecValTok{15}\NormalTok{) }\SpecialCharTok{+}
  \FunctionTok{geom\_point}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{y=}\NormalTok{price)) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{x=}\StringTok{\textquotesingle{}AGE\textquotesingle{}}\NormalTok{,}\AttributeTok{y=}\StringTok{\textquotesingle{}PRICE\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{lecture12_files/figure-latex/unnamed-chunk-3-1.pdf}

I briefly mentioned that an alternative model, which does not include
the assumption of multiplicative error, would be:

\[
\text{PRICE}_i =\exp{(\beta_0 + \beta_1 \text{AGE}_i)} + \epsilon_i
\]

Where \(\epsilon\) is some suitable random variable with mean 0.

Such a model would sidestep the increasing variance: \[
\begin{split}
\text{Var}[\text{PRICE}_i] &= \text{Var}\left[ \exp{ \left( \beta_0 + \beta_1 \text{AGE}_i \right)} +\epsilon _i  \right]\\
&= \text{Var}\left[ \epsilon _i  \right]\\
\end{split}
\]

And furthermore would have interpretation that we are used to for linear
regression (that is, thinking in terms of ``average effects''): \[
E[\text{PRICE}_i] = \exp{ \left( \beta_0 + \beta_1 \text{AGE}_i \right) }
\]

So why don't we use this model instead? Well, it's not linearizable,
that is we can't write in the form: \[
y_i = \vec{x}_i^T \vec{\beta} + \epsilon_i
\]

For any definition of \(x_i\) and \(y_i\).

Why is linearity of model so important? Well, the form of a linear model
was essential to us being able to perform maximum likelihood estimation.
Recall that (from the normality of \(\epsilon_i\)) we were able to write
the log-likelihood: \[
l(\vec{\beta}; \vec{x}_i, y_i) \propto -\sum_i (y_i - \vec{x}_i^T \vec{\beta})^2
\]

And that we were able to maximize this function \emph{analytically};
that is, we could take the derivative with respect to each element of
\(\vec{\beta}\), set it to 0, and then solve.

Let's look at the likelihood for the model: \[
\text{PRICE}_i = \exp{\left(\beta_0 + \beta_1 \text{AGE}_i\right)} + \epsilon_i
\]

Where \(\epsilon_i\) is normally distributed with variance \(\sigma^2\).

Immediately we see that: \[
\text{PRICE}_i \sim N(\exp{\left(\beta_0 + \beta_1 \text{AGE}_i\right)}, \sigma^2)
\]

Thus the likelihood for the full dataset is: \[
L(\beta_0, \beta_1; \text{AGE}_i, \text{PRICE}_i) = \prod\limits_{i=1}^n  N(\exp{\left(\beta_0 + \beta_1 \text{AGE}_i\right)}, \sigma^2)
\]

And so when we convert to the log-likelihood scale, we get: \[
l(\beta_0, \beta_1; \text{AGE}_i, \text{PRICE}_i) = \propto -\sum_i (y_i - \exp{\left(\beta_0 + \beta_1 \text{AGE}_i\right)})^2
\]

Can we still find values for \(\beta_0\) and \(\beta_1\) which maximize
\(l(\beta_0, \beta_1; \text{AGE}_i, \text{PRICE}_i)\)? Well, not
analytically (try setting the derivative to 0 and solving!), and if this
were 1972 that would be basically the end of it. However, in 2021 we
have gotten \textbf{very} good at \emph{numerical optimization}.

Numerical optimization is a field of math which studies how to
\emph{approximately} maximize or minimize a function in some variables.
Rather than trying to take the derivative and solve for 0, numerical
optimization applies iterative algorithms to produce a series of guesses
at the function inputs which maximize the function output. Ideally after
running these algorithms for a number of loops, we wind up with a
something very close to the true maximizing input values.

We won't be covering numerical optimization in any detail in this course
(see the Appendix for a quick overview of a core algorithm, the
Newton-Raphson method), however we will be using it quite a lot. Pretty
much any statistical program worth its salt provides some kind of
functionality for performing numerical optimization. In R one such
function is \texttt{optim}.

Let's see how we could use numerical optimization to fit our
non-linearizable model:

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# by default, optim minimizes the input function so work on the negative log{-}likelihood scale}
\NormalTok{neg.log.likelihood }\OtherTok{=} \ControlFlowTok{function}\NormalTok{(b,x,y)\{}
\NormalTok{  b0 }\OtherTok{=}\NormalTok{ b[}\DecValTok{1}\NormalTok{]}
\NormalTok{  b1 }\OtherTok{=}\NormalTok{ b[}\DecValTok{2}\NormalTok{]}
  
\NormalTok{  y.hat }\OtherTok{=} \FunctionTok{exp}\NormalTok{(b0 }\SpecialCharTok{+}\NormalTok{ b1}\SpecialCharTok{*}\NormalTok{x)}
\NormalTok{  resids }\OtherTok{=}\NormalTok{ y }\SpecialCharTok{{-}}\NormalTok{ y.hat}
\NormalTok{  sse }\OtherTok{=} \FunctionTok{sum}\NormalTok{(resids}\SpecialCharTok{\^{}}\DecValTok{2}\NormalTok{)}
  
  \FunctionTok{return}\NormalTok{(sse)}
\NormalTok{\}}

\NormalTok{age }\OtherTok{=}\NormalTok{ wine}\SpecialCharTok{$}\NormalTok{age}
\NormalTok{price }\OtherTok{=}\NormalTok{ wine}\SpecialCharTok{$}\NormalTok{price}

\NormalTok{first.guess }\OtherTok{=} \FunctionTok{c}\NormalTok{(}\DecValTok{0}\NormalTok{,}\DecValTok{0}\NormalTok{)}
\NormalTok{mle }\OtherTok{=} \FunctionTok{optim}\NormalTok{(first.guess,}
\NormalTok{            neg.log.likelihood,}
            \AttributeTok{x=}\NormalTok{age,}
            \AttributeTok{y=}\NormalTok{price)}

\NormalTok{mle}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
## $par
## [1] 1.30229044 0.03186766
## 
## $value
## [1] 70.35076
## 
## $counts
## function gradient 
##       79       NA 
## 
## $convergence
## [1] 0
## 
## $message
## NULL
\end{verbatim}

This output is telling us that (among other things) our optimization
concluded successfully (\texttt{convergence=0}), and that the MLEs for
this model are approximately \(\hat{\beta_0}=1.3\) and
\(\hat{\beta}_1=.03\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{beta0.hat }\OtherTok{=}\NormalTok{ mle}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{1}\NormalTok{]}
\NormalTok{beta1.hat }\OtherTok{=}\NormalTok{ mle}\SpecialCharTok{$}\NormalTok{par[}\DecValTok{2}\NormalTok{]}

\NormalTok{price.hat }\OtherTok{=} \FunctionTok{exp}\NormalTok{(beta0.hat }\SpecialCharTok{+}\NormalTok{ beta1.hat}\SpecialCharTok{*}\NormalTok{age)}

\FunctionTok{plot}\NormalTok{(age,price,}\AttributeTok{xlab=}\StringTok{\textquotesingle{}AGE\textquotesingle{}}\NormalTok{,}\AttributeTok{ylab=}\StringTok{\textquotesingle{}PRICE\textquotesingle{}}\NormalTok{)}
\FunctionTok{lines}\NormalTok{(age,price.hat)}
\FunctionTok{lines}\NormalTok{(age,}\FunctionTok{exp}\NormalTok{(}\FunctionTok{predict}\NormalTok{(mod.log)),}\AttributeTok{col=}\StringTok{\textquotesingle{}red\textquotesingle{}}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{lecture12_files/figure-latex/unnamed-chunk-5-1.pdf}

In this example, the two models (additive or multiplicative error) have
very similar predictions. In general, this may not be the case.

\hypertarget{generalized-linear-models}{%
\section{Generalized Linear Models}\label{generalized-linear-models}}

The model that we have just fit belongs to a larger class of statistical
models called \emph{Generalized Linear Models}.

Given a dependent variable \(y_i\), and a vector independent variables
\(\vec{x}_i\), a generalized linear model is any model which can be
written in the form: \[
g(E[y_i]) = \vec{x}_i^T \vec{\beta}
\] Or, equivalently: \[
E[y_i] = g^{-1}(\vec{x}_i^T \vec{\beta})
\] In our previous example the function \(g(y) = \log{(y)}\)

\textbf{NOTE:} generalized linear models are different from the
linearizing transformations we saw last week. Among other things, a
linearizing transformation \(g'\) is applied to the observations \(y_i\)
direcly, whereas here it is being applied to the expected value
\(E[y_i]\). These are generally not the same thing (see the example
above).

The function \(g\) is known as the \textbf{link function} and it,
alongside the choice of distribution for the \(y_i\) completely defines
a particular choice of generalized linear model (GLM):

\begin{longtable}[]{@{}
  >{\raggedleft\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.41}}
  >{\centering\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.32}}
  >{\raggedright\arraybackslash}p{(\columnwidth - 4\tabcolsep) * \real{0.27}}@{}}
\toprule
GLM Name & \(g\) & Distribution of \(y_i\) \\ \addlinespace
\midrule
\endhead
Linear Regression & \(g(z)=z\) & Normal \\ \addlinespace
Logistic Regression & \(g(z) = \text{logit}(z)\) &
Binomial \\ \addlinespace
Poisson Regression & \(g(z)=\log{(z)}\) & Poisson \\ \addlinespace
Neural Network* & \(g(z) = \text{a stack of perceptrons}\) & Normal (or
Laplace maybe) \\ \addlinespace
\bottomrule
\end{longtable}

* \emph{technically this is a Generalized} \textbf{Additive} \emph{Model
(GAM), which is a special type of GLM that we'll talk about near the end
of the GLM section of the course}

\hypertarget{the-exponential-family-of-distributions}{%
\section{The Exponential Family of
Distributions}\label{the-exponential-family-of-distributions}}

Broadly speaking, basically any function \(g\) can be a link function.
The primary limits here are:

\begin{enumerate}
\def\labelenumi{\alph{enumi}.}
\tightlist
\item
  \emph{What is interpretable}
\item
  \emph{What is computationally tractable}
\end{enumerate}

More limiting here is the density function of the \(y_i\) (what we can
think of as our ``error distribution''). Specifically, we need to pick
something that belongs to the \textbf{exponential family of
distributions}.

\hypertarget{definition}{%
\subsection{Definition}\label{definition}}

A member of the exponential family of distributions if it's density
function can be written as:

\[
\begin{split}
f(y;\theta) &= s(y) t(\theta) \exp{ \left[ a(y) b(\theta) \right]}\\
&= \exp{ \left[ a(y) b(\theta) + c(\theta) + d(y)\right]}\\
\end{split}
\] Where \(a(y)\), \(b(\theta)\), \(s(y) = \exp{d(y)}\) and
\(t(\theta) = \exp{c(\theta)}\) are basically any functions (be
reasonable). If \(a(y)=y\) then \(b(\theta)\) is called the
\emph{natural parameter} and the function is in \emph{canonical form}.

\hypertarget{properties}{%
\subsection{Properties}\label{properties}}

A nice thing that's true for all probability distributions is:

\[
\int f(y;\theta) dy = 1
\] (Swap integral for sum if \(y\) is discrete)

By taking the derivative \(\frac{d}{d \theta}\) on both sides, we see
that an immediate consequence of this is :

\[
\begin{split}
\frac{d}{d \theta} \int f(y,\theta) dy &= \frac{d}{d \theta} 1\\
\int \frac{d}{d \theta}  f(y,\theta) dy &= 0 \\
\end{split}
\]

Applying this result to the exponential family gives us a nifty (and
useful!) result:

\[
\begin{split}
\frac{d f(y;\theta)}{d \theta} = [a(y) b'(\theta) + c'(\theta)] f(y;\theta)
\end{split}
\] Integrating both sides with respect to \(y\) gives us:

\[
\begin{split}
\int [a(y) b'(\theta) + c'(\theta)] f(y;\theta) dy =  E[a(y)|\theta] b'(\theta) + c'(\theta) = 0
\end{split}
\] And thus:

\[
E[a(y)] = \frac{ -c'(\theta)}{b'(\theta)}
\] A similar argument can be used to find an expression for
\(\text{Var}[a(y)]\) (though it's kind of grody, see IGLM page 49).

The real reason that \textbf{we} care about the exponential family is
because of it's log likelihood. Observe that (for a single data point)
the log-liklihood is simply:

\[
l(\theta; y) = \log{f(y;\theta)} = a(y)b(\theta) + c(\theta) + d(y)
\]

Even more importantly, we can compute the \emph{derivative} of the
log-likelihood:

\[
\frac{d}{d \theta} l(\theta; y) = a(y)b'(\theta) + c'(\theta)
\]

This relationship is very important, because recall that we want to
\emph{maximize} the log-likelihood, which occurs at the point
\(\hat{\theta}\) when:

\[
\frac{d}{d \theta} l(\hat{\theta}; y) = a(y)b'(\hat{\theta}) + c'(\hat{\theta}) = 0
\] Indeed, this relationship is \emph{so} important that we will give
the derivative \(\frac{d}{d \theta} l(\theta; y)\) it's own name,
\textbf{the score function}:

\[
U(\theta; y) = \frac{d}{d \theta} l(\theta; y) = a(y)b'(\theta) + c'(\theta)
\] An interesting this about the score function, is that through its
dependence on the random variable \(y\), it is \emph{also} a random
variable (much like how \(\hat{\beta}\) was a random variable when
performing inference for linear regression). This (and all of the other
properties mentioned today) is going to come up next lecture, when we
talk about how we can perform inference with GLMs.

\hypertarget{binomial-distribution}{%
\section{Binomial Distribution}\label{binomial-distribution}}

Imagine you have a coin that, if flipped, lands ``heads'' p\% of the
time. If you flip the coin \(n\) times, what is the probability of
getting \(k\) ``heads'' outcomes?

This is the interpretation of the Binomial Distribution:

\[
\text{Binom}(k;p,n) = {n\choose k} p^k (1-p)^{n-k}
\]

The Binomial distribution is one of the most common distributions you'll
see in statistical modeling (as well as Machine Learning), as it is the
``error distribution'' for logistic regression.

When \(n=1\) then the the binomial distribution is sometimes referred to
as the ``Bernoulli'' distribution.

Here we will look at showing that the binomial is in the exponential
family, as well as showing a few of the properties outlined above.

\hypertarget{exponential-family}{%
\subsection{Exponential Family}\label{exponential-family}}

Let's use \(f(y;\theta)\) to denote the binomial PMF (where
\(\theta = p\) and \(y=k\)): \[
f(y;\theta) = {n \choose y} \theta^y (1-\theta)^{n-y}
\] Some algebra then gives us:

\$\$

\begin{split}
f(y;\theta) &= \exp{ \left[ \log{ \left( {n \choose y} \theta^y (1-\theta)^{n-y} \right) } \right] }\\
&= \exp{ \left[ \log{ {n \choose y}} + y \log{(\theta)} + (n-y) \log{(1-\theta)} \right]}\\
&= \exp{ \left[  y (\log{(\theta)} - \log{(1-\theta)}) + n\log{(1-\theta)}+ \log{ {n \choose y}} \right]}\\
&= \exp{ \left[  y \log{\frac{\theta}{1-\theta}} + n\log{(1-\theta)}+ \log{ {n \choose y}} \right]}\\
&= \exp{ \left[  a(y) b(\theta) + c(\theta)+ d(y) \right]}\\

\end{split}

\$\$

Since \(a(y)=y\) here we see that the ``natural parameter'' for the
binomial distribution is \(b(\theta) = \log{\frac{\theta}{1-\theta}}\).
This natural parameter is referred to as the log-odds of the
distribution (since it is the logarithm of the odds ratio) and will be
\textbf{super important} next week when we talk about logistic
regression.

\hypertarget{properties-1}{%
\subsection{Properties}\label{properties-1}}

Let's just quickly take a peek at some of the properties of the binomial
PMF as a member of the exponential family.

First, let's see: \[
E[a(y)] = \frac{-c'(\theta)}{b'(\theta)}
\] Since the exponential form of the binomial distribution is canonical,
then this is the same as looking at \(E[y]\).

It's not hard to convince yourself that the expected value of the
binomial distribution is:

\[
E[y] = n \theta
\] So we want to show that: \[
n\theta = \frac{-c'(\theta)}{b'(\theta)}
\] Let's compute the derivatives of \(b\) and \(c\):

\[
\begin{split}
b'(\theta) &= \frac{d}{d \theta} \log {\frac{\theta}{1 - \theta} }\\
&= \frac{d}{d \theta} [ \log{\theta} - \log{(1 - \theta)}]\\
&= \frac{1}{\theta}  - \frac{-1}{1 - \theta}\\
&= \frac{1}{\theta(1-\theta)}
\end{split}
\] \[
\begin{split}
c'(\theta) &= \frac{d}{d \theta} n\log {(1-\theta)}\\
&= n \frac{d}{d \theta} \log {(1-\theta)}\\
&= \frac{-n}{(1-\theta)}\\
\end{split}
\] Thus:

\[
\begin{split}
\frac{-c'(\theta)}{b(\theta)} &= \frac{n/(1-\theta)}{1/\theta(1-\theta)} \\
&= n\theta\\
\end{split}
\]

Let's also quickly look at the log-likelihood for the Binomial
distribution with a single observation: \[
L(\theta;y) = {n \choose y} \theta^y (1-\theta)^{n-y}
\] And thus: \[
l(\theta;y) = y \log{\frac{\theta}{1-\theta}} + n\log{(1-\theta)} + \log{n \choose y}  =a(y)b(\theta) + c(\theta) + d(y)
\] And the score function also gets the form you'd expect: \[
U(\theta;y) = \frac{y}{\theta(1-\theta)} + \frac{n}{1-\theta} = \frac{y + n\theta}{\theta (1-\theta)}
\] It's not an accident that \(E[y] = n \theta\) show up in the above
expression! Since \(E[y] = \frac{-c'(\theta)}{b'(\theta)}\) we also have
\(c'(\theta) = E[y] b'(\theta)\), so the score function can be written:

\[
U(\theta;y) = b'(\theta) (y - E[y])
\] \emph{O\textasciitilde o\textasciitilde o\textasciitilde h} that's
sure suggestive of \emph{something}! Boy I wonder where this is
going\ldots{}

\hypertarget{appendix-a-the-newton-raphson-algorithm}{%
\section{Appendix A: The Newton-Raphson
Algorithm}\label{appendix-a-the-newton-raphson-algorithm}}

Newtown-Raphson (NR) is an algorithm to find a \emph{root} of a function
\(f(y)\). A root is any point \(\hat{y}\) where \(f(\hat{y})=0\).

The NR algorithm does this by recursively computing a sequence of
improved ``guesses'':

\[
y_{m+1} = y_m - \frac{f(y_m)}{f'(y_m)}
\] As \(m\) grows large, \(|f(y_m)|\) goes to 0 \emph{quadratically},
that is the error at the updated point \(y_{m+1}\) decays as: \[
|\hat{y} - y_{m+1}| \propto |\hat{y} - y_m|^2
\]

The right hand side of the recursion equation comes from a Taylor
approximation. Recall that any function can be approximated by truncated
series of polynomials: \[
f(y) \approx f(a) + f'(a)(y-a) + f''(a)(y-a)^2 + ...
\]

If we set \(a=y\) and \(y=\hat{y}\), then at first order we have:

\[
\begin{split}
f(\hat{y}) \approx f(y) + f'(y)(\hat{y}-y) \\
0 \approx f(y) + f'(y)(\hat{y}-y) \\
\end{split}
\] We can then ``solve'' this approximate equation for \(\hat{y}\) to
get: \[
\begin{split}
-f'(y)(\hat{y}-y) &\approx f(y)\\
\hat{y} - y &\approx \frac{-f(y)}{f'(y)}\\
\hat{y} &\approx y - \frac{f(y)}{f'(y)}\\
\end{split}
\] Now, although this equivalance only hold for values of \(y\) close to
\(\hat{y}\), by applying it recursively we ultimately improve our
approximation.

Although presented here as a root-finding algorithm, NR is most commonly
applied to find a (local) max or min of a function \(g(y)\). To do so we
simply apply the NR algorithm to the \emph{derivative} \(f(y) = g'(y)\).
In optimization contexts the recursion algorithm is sometimes presented
in terms of \(g\), rather than \(f\):

\[
y_{m+1} = y_m - \frac{g'(y_m)}{g''(y_m)}
\]

In GLM applications NR is presented in terms of the \emph{score
function}:

\[
\theta_{m+1} = \theta_m - \frac{U(\theta)}{U'(\theta)}
\]

Where the first derivative of the score function \(U'\) (the second
derivative of the likelihood) is sometimes referred to as the
\emph{information}.

\end{document}
